{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 1976042829876791,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "module_path = os.path.abspath(os.path.join('..'))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "module_path = os.path.abspath(os.path.join('../../'))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 1507707803701025,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from enum import Enum\n",
        "from libfb.py.pyre import none_throws\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "import callm.core.utils.utils as utils\n",
        "from callm.metaformers.src.args.trainer import TrainerArgs\n",
        "from callm.core.data import datautils\n",
        "from callm.core.model_utils import get_mp_rank_size, get_consolidated_ckpt_path, ElasticQuantBinarizerSigned, get_torch_dtype\n",
        "from callm.core.models.llama_xl.transformer import (\n",
        "    Transformer,\n",
        "    TransformerForCausalLM,\n",
        "    TransformerForSequenceClassification,\n",
        "    wrap_model,\n",
        "    wrap_model_pt,\n",
        ")\n",
        "import llama_xl.quantized_transformer as quantized_transformer\n",
        "from callm.core.utils.process_args import (\n",
        "    QAT,\n",
        ")\n",
        "from fairscale.nn.model_parallel import initialize as fs_init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 1962347531244174,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from torch.distributed.elastic.utils.distributed import get_free_port\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
        "os.environ[\"MASTER_PORT\"] = str(get_free_port())\n",
        "os.environ[\"RANK\"] = \"0\"\n",
        "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
        "\n",
        "# Initialize the basic distributed process group\n",
        "dist.init_process_group(backend='nccl')  # or 'gloo' for CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 24104645639206716,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "import fairscale.nn.model_parallel.initialize as fs_init\n",
        "\n",
        "# Initialize model parallel groups using fairscale\n",
        "if not fs_init.model_parallel_is_initialized():\n",
        "    fs_init.initialize_model_parallel(\n",
        "        1,\n",
        "        model_parallel_backend=\"nccl\",\n",
        "        ddp_backend=\"nccl\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 1096836112079939,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "# Load the FP model and compute the activations on the Wiki2 dataset\n",
        "class ParallelImpl(str, Enum):\n",
        "    FAIRSCALE = \"FAIRSCALE\"\n",
        "    PT_D = \"PT_D\"\n",
        "    NONE = \"NONE\"\n",
        "\n",
        "class data_args:\n",
        "    checkpoint_path = \"xx/1_16_16/paretoq_lr_2e5/70000/\"\n",
        "    max_parallel_files = 1\n",
        "    api_key = None\n",
        "\n",
        "class model_args:\n",
        "    input_model_filename = \"xx/baselines/full_precision_models/original_xl/llama/1B/\"\n",
        "    input_model_local_path = None\n",
        "    parallel_impl = ParallelImpl.FAIRSCALE\n",
        "    # model args\n",
        "    share_embedding = True\n",
        "    layer_sharing = False\n",
        "    custom_bwd = True\n",
        "    dropout = 0\n",
        "    w_bits = 1\n",
        "    a_bits = 16\n",
        "    kv_bits = 16\n",
        "    emb_bits = 32\n",
        "    output_w_bits = 32\n",
        "    output_a_bits = 32\n",
        "\n",
        "class training_args:\n",
        "    qat = QAT.EXPERIMENTAL\n",
        "    bf16 = True\n",
        "    fp16 = False\n",
        "    model_max_length = 2048\n",
        "\n",
        "# Step 1: Load the FP mode\n",
        "def load_model_xl(\n",
        "    local_rank: int = 0,\n",
        "    wrap: bool = False,\n",
        "    generate_only: bool = False):\n",
        "    pathmgr = utils.get_path_manager(\n",
        "        max_parallel=data_args.max_parallel_files,\n",
        "        api_key=data_args.api_key,\n",
        "    )\n",
        "    model_args.input_model_local_path = pathmgr.get_local_path(\n",
        "        none_throws(model_args.input_model_filename)\n",
        "    )\n",
        "    with pathmgr.open(\n",
        "        os.path.join(model_args.input_model_local_path, \"params.json\")\n",
        "    ) as f:\n",
        "        data = json.load(f)\n",
        "        if \"hive_data\" in data:\n",
        "            del data[\"hive_data\"]\n",
        "        args = TrainerArgs.from_dict(data)\n",
        "        args.dtype = \"bf16\" if training_args.bf16 else \"fp16\"\n",
        "        args.model.sequence_parallel = False\n",
        "        args.model.loss_parallel = False\n",
        "        args.model.max_length = none_throws(training_args.model_max_length)\n",
        "        # pyre-fixme[16]: `ModelArgs` has no attribute `share_embedding`.\n",
        "        args.model.share_embedding = none_throws(model_args.share_embedding)\n",
        "        # pyre-fixme[16]: `ModelArgs` has no attribute `layer_sharing`.\n",
        "        args.model.layer_sharing = none_throws(model_args.layer_sharing)\n",
        "        args.model.parallel_impl = none_throws(model_args.parallel_impl)\n",
        "        args.model.custom_bwd = (\n",
        "            none_throws(model_args.custom_bwd)\n",
        "            and not generate_only\n",
        "            and args.model.parallel_impl != ParallelImpl.NONE\n",
        "        )\n",
        "        if model_args.dropout > 0:\n",
        "            args.model.dropout = model_args.dropout\n",
        "            args.model.custom_bwd = False\n",
        "\n",
        "    consolidated_ckpt_local_path = None\n",
        "    if data_args.checkpoint_path is not None:\n",
        "        mp_rank, mp_size = get_mp_rank_size()\n",
        "        consolidated_ckpt_local_path = pathmgr.get_local_path(\n",
        "            get_consolidated_ckpt_path(\n",
        "                none_throws(data_args.checkpoint_path),\n",
        "                mp_rank,\n",
        "                mp_size,\n",
        "                none_throws(model_args.parallel_impl),\n",
        "            )\n",
        "        )\n",
        "\n",
        "    config = transformers.AutoConfig.from_pretrained(model_args.input_model_local_path)\n",
        "    args.model.vocab_size = config.vocab_size\n",
        "\n",
        "    assert data_args.checkpoint_path is not None\n",
        "    args.model.init.no_init = True\n",
        "\n",
        "    if training_args.qat == QAT.EXPERIMENTAL:\n",
        "        args.model.emb_bits = model_args.emb_bits\n",
        "        args.model.output_w_bits = model_args.output_w_bits\n",
        "        args.model.output_a_bits = model_args.output_a_bits\n",
        "        args.model.kv_bits = model_args.kv_bits\n",
        "        args.model.w_bits = model_args.w_bits\n",
        "        args.model.a_bits = model_args.a_bits\n",
        "        model = quantized_transformer.Transformer(args.model)\n",
        "    else:\n",
        "        model = Transformer(args.model)\n",
        "\n",
        "    if model_args.parallel_impl != ParallelImpl.PT_D:\n",
        "        assert consolidated_ckpt_local_path is not None\n",
        "        state_dict = torch.load(consolidated_ckpt_local_path, map_location=\"cpu\")\n",
        "\n",
        "        for key in model.state_dict().keys():\n",
        "            if key not in state_dict.keys() and \"weight_clip_val\" in key:\n",
        "                weight_key = key.replace(\"weight_clip_val\", \"weight\")\n",
        "                x = state_dict[weight_key]\n",
        "                best = torch.full(\n",
        "                    [x.shape[0], 1], float(\"inf\"), device=x.device\n",
        "                ).type_as(x)\n",
        "                grid = 100\n",
        "                norm = 2.4\n",
        "                if model_args.w_bits == 1:\n",
        "                    scale = (torch.mean(x.abs(), dim=-1, keepdim=True)).detach()\n",
        "                    state_dict[key] = scale\n",
        "                    continue\n",
        "                if model_args.w_bits == 0 or model_args.w_bits == 2:\n",
        "                    scale, _ = torch.max(torch.abs(x), dim=-1, keepdim=True)\n",
        "                    state_dict[key] = scale\n",
        "                    continue\n",
        "                if model_args.w_bits >= 3:\n",
        "                    xmax, _ = torch.max(torch.abs(x), dim=-1, keepdim=True)\n",
        "                    maxshrink = 0.5\n",
        "                else:\n",
        "                    raise NotImplementedError\n",
        "\n",
        "                maxq = 2 ** (model_args.w_bits - 1) - 1\n",
        "                scale = xmax / maxq\n",
        "\n",
        "                state_dict[key] = scale\n",
        "\n",
        "        print(model.load_state_dict(\n",
        "            state_dict,\n",
        "            strict=False,\n",
        "        ))\n",
        "        del state_dict\n",
        "\n",
        "    if training_args.qat == QAT.EXPERIMENTAL:\n",
        "        model = model.cuda().to(get_torch_dtype(args.dtype))\n",
        "        print(\"Loading QAT model\")\n",
        "        model = quantized_transformer.TransformerForCausalLM(config, model)\n",
        "    else:\n",
        "        model_cls = TransformerForCausalLM\n",
        "        model = model.cuda().to(get_torch_dtype(args.dtype))\n",
        "        model = model_cls(config, model)\n",
        "\n",
        "    print(\"finish loading checkpoint and wrap model...\")\n",
        "    if local_rank == 0:\n",
        "        print(model)\n",
        "        print(\"Complete model loading...\")\n",
        "\n",
        "    return model\n",
        "\n",
        "model = load_model_xl()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 1454980975727744,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "# Step 2 Create a wiki2 dataset\n",
        "model_args.llama_version = \"3\"\n",
        "model_args.use_fast_tokenizer = True\n",
        "training_args.cache_dir = None\n",
        "data_args.add_eos_token = False\n",
        "data_args.add_bos_token = False\n",
        "data_args.data_path = \"xx/data_third_party/wiki/wikitext-2/train.jsonl\"\n",
        "data_args.eval_data_path = \"xx/data_third_party/eval/\"\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer_class = transformers.AutoTokenizer\n",
        "if \"3\" in model_args.llama_version:\n",
        "    tokenizer_class = transformers.LlamaTokenizerFast\n",
        "tokenizer = tokenizer_class.from_pretrained(\n",
        "    pretrained_model_name_or_path=model_args.input_model_local_path,\n",
        "    cache_dir=training_args.cache_dir,\n",
        "    model_max_length=training_args.model_max_length,\n",
        "    padding_side=\"right\",\n",
        "    use_fast=model_args.use_fast_tokenizer,\n",
        "    # When evaluating ppl tasks, set add_eos_token to True, otherwise, set it to False.\n",
        "    add_eos_token=data_args.add_eos_token,\n",
        "    add_bos_token=data_args.add_bos_token,\n",
        ")\n",
        "\n",
        "# Load dataset\n",
        "pathmgr = utils.get_path_manager(\n",
        "        max_parallel=data_args.max_parallel_files,\n",
        "        api_key=data_args.api_key,\n",
        "    )\n",
        "data_args.train_data_local_path = pathmgr.get_local_path(\n",
        "        data_args.data_path\n",
        "    )\n",
        "data_args.eval_data_local_path = pathmgr.get_local_path(\n",
        "                data_args.eval_data_path\n",
        "            )\n",
        "train_dataset, valid_dataset = datautils.get_train_val_dataset(\n",
        "        train_path=data_args.train_data_local_path,\n",
        "        valid_path=( None\n",
        "            # os.path.join(data_args.eval_data_local_path, \"wiki2/test.jsonl\")\n",
        "            # if data_args.eval_data_local_path is not None\n",
        "            # else None\n",
        "        ),\n",
        "    )\n",
        "train_data = datautils.CustomJsonDataset(\n",
        "    train_dataset,\n",
        "    tokenizer,\n",
        "    block_size=min(training_args.model_max_length, 2048),\n",
        ")\n",
        "valid_data = datautils.CustomJsonDataset(\n",
        "    valid_dataset,\n",
        "    tokenizer,\n",
        "    block_size=min(training_args.model_max_length, 2048),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from callm.core.models.llama.utils_quant import QuantizedEmbedding, QuantizeLinear\n",
        "from callm.core.models.llama_xl.quantized_layers import QuantizedParallelEmbedding, QuantizedColumnParallelLinear, QuantizedRowParallelLinear\n",
        "from collections import defaultdict\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "device = model.device\n",
        "\n",
        "def move_batch_to_device(batch, device):\n",
        "    for key in ['input_ids', \"labels\"]:\n",
        "        if type(batch[key]) == list:\n",
        "            batch[key] = torch.tensor(np.array(batch[key]), dtype=torch.int64).to(device)\n",
        "        if len(batch[key].shape) == 1:\n",
        "            batch[key] = batch[key].view(1, -1)\n",
        "    return batch\n",
        "\n",
        "def evaluate_loss(model, dataset, device, maximum_count = 500):\n",
        "    losses = []\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataset:\n",
        "            batch = move_batch_to_device(batch, device)\n",
        "            outputs = model(input_ids=batch['input_ids'], labels=batch['labels'])\n",
        "            losses.append(outputs.loss.cpu().item())\n",
        "            count += 1\n",
        "\n",
        "            if count >= maximum_count:\n",
        "                break\n",
        "    return np.mean(losses), np.std(losses)\n",
        "\n",
        "\n",
        "def weighted_average(x, count):\n",
        "    loss = np.sum(x * count)\n",
        "    total =  np.sum(count)\n",
        "    return (loss / total), total\n",
        "\n",
        "\n",
        "def avg_dist_dict(keys, dictionary):\n",
        "    avg = {}\n",
        "    for k in keys:\n",
        "        v = dictionary[k]\n",
        "        if len(v) > 0:\n",
        "            avg_v = float(np.mean(v))\n",
        "        else:\n",
        "            avg_v = 0.0\n",
        "        try:\n",
        "            dist_avg_v, _ = weighted_average(avg_v, len(v))\n",
        "        except ZeroDivisionError:\n",
        "            dist_avg_v = -1\n",
        "        avg[k] = dist_avg_v\n",
        "    return avg\n",
        "\n",
        "def eval_ppl(model, dataset, device, maximum_count = 500):\n",
        "    model.eval()\n",
        "    count = 0\n",
        "    metrics_ls = defaultdict(list)\n",
        "    with torch.no_grad():\n",
        "        for batch in dataset:\n",
        "            batch = move_batch_to_device(batch, device)\n",
        "            outputs = model(input_ids=batch['input_ids'], labels=batch['labels'])\n",
        "            logits = outputs.logits[:, :-1]\n",
        "\n",
        "            y = batch[\"labels\"][:, 1:].to(device)\n",
        "            loss = F.cross_entropy(logits.flatten(0, 1), y.flatten(0, 1), reduction=\"sum\")\n",
        "            metric = loss.item()\n",
        "            n_toks = y.nelement()\n",
        "            metrics_ls[\"metric\"].append(metric)\n",
        "            metrics_ls[\"n_toks\"].append(n_toks)\n",
        "\n",
        "            count += 1\n",
        "\n",
        "            if count >= maximum_count:\n",
        "                break\n",
        "    metrics = avg_dist_dict([\"metric\", \"n_toks\"], metrics_ls)\n",
        "    ppl = math.exp(metrics[\"metric\"] / metrics[\"n_toks\"])\n",
        "    return ppl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "state_dict = model.model.state_dict()\n",
        "state_dict = {key: val.detach().cpu() for key, val in state_dict.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 736398412600381,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "fp_model_loss, fp_model_loss_std = evaluate_loss(model, train_data, device, maximum_count = 200)\n",
        "print(\"FP model loss: {:.4f} +/- {:.4f}\".format(fp_model_loss, fp_model_loss_std))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from llama.utils_quant import QuantizedEmbedding, QuantizeLinear\n",
        "from llama_xl.quantized_layers import (\n",
        "    QuantizedColumnParallelLinear,\n",
        "    QuantizedParallelEmbedding,\n",
        "    QuantizedRowParallelLinear,\n",
        ")\n",
        "\n",
        "\n",
        "def prepare_inputs(batch, device):\n",
        "    \"\"\"\n",
        "    prepare inputs for model\n",
        "    \"\"\"\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    return batch\n",
        "\n",
        "\n",
        "def normalization(vs):\n",
        "    \"\"\"\n",
        "    normalization of a list of vectors\n",
        "    return: normalized vectors v\n",
        "    \"\"\"\n",
        "    norms = [torch.sum(v * v) for v in vs]\n",
        "    norms = [(norm**0.5).cpu().item() for norm in norms]\n",
        "    vs = [vi / (norms[i] + 1e-6) for (i, vi) in enumerate(vs)]\n",
        "    return vs\n",
        "\n",
        "\n",
        "def orthnormal(ws, vs_list):\n",
        "    \"\"\"\n",
        "    make vector w orthogonal to each vector in v_list.\n",
        "    afterwards, normalize the output w\n",
        "    \"\"\"\n",
        "    for vs in vs_list:\n",
        "        for w, v in zip(ws, vs):\n",
        "            w.data.add_(-v * (torch.sum(w * v)))\n",
        "    return normalization(ws)\n",
        "\n",
        "\n",
        "def get_layers(\n",
        "    module,\n",
        "    layers=[\n",
        "        torch.nn.Linear,\n",
        "        QuantizeLinear,\n",
        "        QuantizedEmbedding,\n",
        "        torch.nn.Embedding,\n",
        "        QuantizedRowParallelLinear,\n",
        "        QuantizedColumnParallelLinear,\n",
        "        QuantizedParallelEmbedding,\n",
        "    ],\n",
        "    name: str = \"\",\n",
        "):\n",
        "    if (\n",
        "        type(module)\n",
        "        in [QuantizedEmbedding, torch.nn.Embedding, QuantizedParallelEmbedding]\n",
        "        and type(module) in layers\n",
        "    ):\n",
        "        return {\"embed_tokens\": module}\n",
        "    if type(module) in layers:\n",
        "        return {name: module}\n",
        "    res = {}\n",
        "    for name1, child in module.named_children():\n",
        "        res.update(\n",
        "            get_layers(\n",
        "                child, layers=layers, name=name + \".\" + name1 if name != \"\" else name1\n",
        "            )\n",
        "        )\n",
        "    return res\n",
        "\n",
        "\n",
        "def compute_eigenvalue(model, loss, device, maxIter=200, tol=1e-3, top_n=1):\n",
        "    \"\"\"Calculate Top Eigenvalue of Hessian\"\"\"\n",
        "    # # Get parameters and gradients of corresponding layer\n",
        "    # batch = prepare_inputs(batch, device)\n",
        "    # outputs = model(**batch)\n",
        "    # loss = outputs.loss\n",
        "\n",
        "    layers = get_layers(model)\n",
        "    weights = [module.weight for name, module in layers.items()]\n",
        "    model.zero_grad()\n",
        "    \"\"\" use negative loss to get the minimum eigenvalue here \"\"\"\n",
        "    gradients = torch.autograd.grad(loss, weights, retain_graph=True, create_graph=True)\n",
        "\n",
        "    topn_eigenvalues = []\n",
        "    eigenvectors = []\n",
        "    computed_dim = 0\n",
        "    while computed_dim < top_n:\n",
        "        eigenvalues = None\n",
        "        vs = [torch.randn_like(weight) for weight in weights]  # generate random vector\n",
        "        vs = normalization(vs)  # normalize the vector\n",
        "\n",
        "        for _ in range(maxIter):\n",
        "            vs = orthnormal(vs, eigenvectors)\n",
        "            model.zero_grad()\n",
        "\n",
        "            Hvs = torch.autograd.grad(\n",
        "                gradients, weights, grad_outputs=vs, retain_graph=True\n",
        "            )\n",
        "            tmp_eigenvalues = [\n",
        "                torch.sum(Hv * v).cpu().item() for (Hv, v) in zip(Hvs, vs)\n",
        "            ]\n",
        "\n",
        "            vs = normalization(Hvs)\n",
        "\n",
        "            if eigenvalues == None:\n",
        "                eigenvalues = tmp_eigenvalues\n",
        "            else:\n",
        "                if (\n",
        "                    abs(sum(eigenvalues) - sum(tmp_eigenvalues))\n",
        "                    / (abs(sum(eigenvalues)) + 1e-6)\n",
        "                    < tol\n",
        "                ):\n",
        "                    break\n",
        "                else:\n",
        "                    eigenvalues = tmp_eigenvalues\n",
        "        topn_eigenvalues.append(eigenvalues)\n",
        "        eigenvectors.append(vs)\n",
        "        computed_dim += 1\n",
        "\n",
        "    return topn_eigenvalues, eigenvectors\n",
        "\n",
        "\n",
        "def compute_hessian_traces(model, loss, device, maxIter=200, tol=1e-4):\n",
        "    # batch = prepare_inputs(batch, device)\n",
        "    # outputs = model(**batch)\n",
        "    # loss = outputs.loss\n",
        "\n",
        "    layers = get_layers(model)\n",
        "    weights = []\n",
        "    for name, module in layers.items():\n",
        "        weights.append(module.weight)\n",
        "    model.zero_grad()\n",
        "    gradients = torch.autograd.grad(loss, weights, retain_graph=True, create_graph=True)\n",
        "\n",
        "    layer_traces = []\n",
        "    trace_vhv = []\n",
        "    trace = 0.0\n",
        "    for _ in range(maxIter):\n",
        "        vs = [torch.randint_like(weight, high=2) for weight in weights]\n",
        "\n",
        "        for v in vs:\n",
        "            v[v == 0] = -1\n",
        "\n",
        "        model.zero_grad()\n",
        "        Hvs = torch.autograd.grad(\n",
        "            gradients, weights, grad_outputs=vs, retain_graph=True\n",
        "        )\n",
        "        tmp_layer_traces = np.array(\n",
        "            [torch.sum(Hv * v).cpu().item() for Hv, v in zip(Hvs, vs)]\n",
        "        )\n",
        "\n",
        "        layer_traces.append(tmp_layer_traces)\n",
        "        trace_vhv.append(sum(tmp_layer_traces))\n",
        "\n",
        "        if abs(np.mean(trace_vhv) - trace) / (abs(trace) + 1e-6) < tol:\n",
        "            break\n",
        "        else:\n",
        "            trace = np.mean(trace_vhv)\n",
        "    return np.mean(np.array(layer_traces), axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_linear_layers(\n",
        "    module,\n",
        "    layers=[\n",
        "        torch.nn.Linear,\n",
        "        QuantizeLinear,\n",
        "        QuantizedRowParallelLinear,\n",
        "        QuantizedColumnParallelLinear,\n",
        "    ],\n",
        "    name: str = \"\",\n",
        "):\n",
        "    if (\n",
        "        type(module)\n",
        "        in [QuantizedEmbedding, torch.nn.Embedding, QuantizedParallelEmbedding]\n",
        "        and type(module) in layers\n",
        "    ):\n",
        "        return {\"embed_tokens\": module}\n",
        "    if type(module) in layers:\n",
        "        return {name: module}\n",
        "    res = {}\n",
        "    for name1, child in module.named_children():\n",
        "        res.update(\n",
        "            get_linear_layers(\n",
        "                child, layers=layers, name=name + \".\" + name1 if name != \"\" else name1\n",
        "            )\n",
        "        )\n",
        "    return res\n",
        "\n",
        "def get_embedding_layers(\n",
        "    module,\n",
        "    layers=[\n",
        "        QuantizedEmbedding,\n",
        "        torch.nn.Embedding,\n",
        "        QuantizedParallelEmbedding,\n",
        "    ],\n",
        "    name: str = \"\",\n",
        "):\n",
        "    if (\n",
        "        type(module)\n",
        "        in [QuantizedEmbedding, torch.nn.Embedding, QuantizedParallelEmbedding]\n",
        "        and type(module) in layers\n",
        "    ):\n",
        "        return {\"embed_tokens\": module}\n",
        "    if type(module) in layers:\n",
        "        return {name: module}\n",
        "    res = {}\n",
        "    for name1, child in module.named_children():\n",
        "        res.update(\n",
        "            get_embedding_layers(\n",
        "                child, layers=layers, name=name + \".\" + name1 if name != \"\" else name1\n",
        "            )\n",
        "        )\n",
        "    return res\n",
        "\n",
        "def get_model_param_list(model):\n",
        "    layers = get_linear_layers(model) # change here\n",
        "    weights = []\n",
        "    for name, module in layers.items():\n",
        "        weights.append(module.weight)\n",
        "    return weights\n",
        "\n",
        "\n",
        "def hvp(model, loss, v, create_graph=False):\n",
        "    \"\"\"\n",
        "    Compute H @ v where H is the Hessian of the training loss w.r.t. parameters.\n",
        "    Uses a single mini-batch to approximate the training loss Hessian.\n",
        "    \"\"\"\n",
        "    param_list = get_model_param_list(model)\n",
        "    grads = torch.autograd.grad(loss, param_list, retain_graph=True, create_graph=True)\n",
        "    g = torch.cat([gi.reshape(-1) for gi in grads])\n",
        "\n",
        "    # form g^T v and take gradient again\n",
        "    gv = (g * v).sum()\n",
        "    Hv = torch.autograd.grad(\n",
        "        gv, param_list, retain_graph=True, create_graph=False\n",
        "    )\n",
        "    Hv = torch.cat([h.reshape(-1) for h in Hv]).detach()\n",
        "    return Hv\n",
        "\n",
        "\n",
        "def lanczos_tridiag(hvp_fn, dim, m, v0=None, device=\"cuda:0\"):\n",
        "    \"\"\"\n",
        "    Run m steps of Lanczos on implicit symmetric operator H using HVPs.\n",
        "    Returns alpha (diag), beta (off-diag), and the first Lanczos vector q1.\n",
        "    \"\"\"\n",
        "    if v0 is None:\n",
        "        v = torch.randn(dim, device=device)\n",
        "    else:\n",
        "        v = v0.clone().detach().to(device)\n",
        "    q = v / (v.norm() + 1e-12)\n",
        "    Q1 = q.clone()  # return q1 for weight computation\n",
        "    alphas = []\n",
        "    betas = []\n",
        "    prev_q = torch.zeros_like(q)\n",
        "\n",
        "    for _ in range(m):\n",
        "        z = hvp_fn(q)\n",
        "        alpha = torch.dot(q, z).item()\n",
        "        z = z - alpha * q - (betas[-1] * prev_q if betas else 0)\n",
        "        beta = z.norm().item()\n",
        "        alphas.append(alpha)\n",
        "        betas.append(beta)\n",
        "        prev_q, q = (\n",
        "            q,\n",
        "            (z / (beta + 1e-12)) if beta > 1e-14 else (z * 0.0),\n",
        "        )  # stop if breakdown\n",
        "        if beta < 1e-14:\n",
        "            break\n",
        "\n",
        "    # betas length is m; last beta is the next-iteration beta (unused in T)\n",
        "    return (\n",
        "        np.array(alphas, dtype=np.float64),\n",
        "        np.array(betas[:-1], dtype=np.float64),\n",
        "        Q1,\n",
        "    )\n",
        "\n",
        "\n",
        "def hvp_flat(model, loss, v_flat):\n",
        "    # map flat vector to parameter-shaped list for autograd\n",
        "    param_list = get_model_param_list(model)\n",
        "    v_slices = []\n",
        "    idx = 0\n",
        "    for p in param_list:\n",
        "        n = p.numel()\n",
        "        v_slices.append(v_flat[idx : idx + n].view_as(p))\n",
        "        idx += n\n",
        "    v = torch.cat(\n",
        "        [vi.reshape(-1) for vi in v_slices]\n",
        "    )  # flat again for our hvp() interface\n",
        "    return hvp(model, loss, v, create_graph=False)\n",
        "\n",
        "\n",
        "def slq_density(model, loss, n_probes=20, m=50, sigma=1e-3, grid=None, device=\"cuda:0\"):\n",
        "    \"\"\"\n",
        "    Returns: grid (np.array), density (np.array), ritz_values_all (list), approx_lam_min/max\n",
        "    \"\"\"\n",
        "    param_list = get_model_param_list(model)\n",
        "    dim = sum(p.numel() for p in param_list)\n",
        "    # We'll accumulate Ritz values to set a data-driven grid after a warmup\n",
        "    grid_min, grid_max = +np.inf, -np.inf\n",
        "    thetas_all = []\n",
        "    weights_all = []\n",
        "\n",
        "    for _ in range(n_probes):\n",
        "        v0 = [torch.randn_like(p) for p in param_list]\n",
        "        v0 = torch.cat([vi.reshape(-1) for vi in v0])\n",
        "        v0 = v0 / (v0.norm() + 1e-12)\n",
        "        alphas, betas, q1 = lanczos_tridiag(\n",
        "            lambda v: hvp_flat(model, loss, v), dim, m, v0=v0\n",
        "        )\n",
        "\n",
        "        # Build T\n",
        "        k = len(alphas)\n",
        "        T = np.diag(alphas[:k])\n",
        "        if k > 1:\n",
        "            off = betas[: k - 1]\n",
        "            T += np.diag(off, 1) + np.diag(off, -1)\n",
        "\n",
        "        # Eigendecompose small tridiagonal\n",
        "        theta, U = np.linalg.eigh(T)\n",
        "        # weights are (first component)^2\n",
        "        w = (U[0, :] ** 2) * (v0.norm().item() ** 2)\n",
        "        thetas_all.append(theta)\n",
        "        weights_all.append(w)\n",
        "        grid_min = min(grid_min, theta.min())\n",
        "        grid_max = max(grid_max, theta.max())\n",
        "\n",
        "    # Construct grid with a small margin\n",
        "    pad = 0.05 * (grid_max - grid_min + 1e-12)\n",
        "    grid = np.linspace(grid_min - pad, grid_max + pad, 400)\n",
        "\n",
        "    # Accumulate Gaussian kernels on the grid\n",
        "    density = np.zeros_like(grid, dtype=np.float64)\n",
        "    lam_mins, lam_maxs = [], []\n",
        "\n",
        "    for i in range(n_probes):\n",
        "        theta = thetas_all[i]\n",
        "        w = weights_all[i]\n",
        "\n",
        "        # Gaussian accumulation\n",
        "        for ti, wi in zip(theta, w):\n",
        "            density += (\n",
        "                wi\n",
        "                * np.exp(-0.5 * ((grid - ti) / sigma) ** 2)\n",
        "                / (np.sqrt(2 * np.pi) * sigma)\n",
        "            )\n",
        "\n",
        "        lam_mins.append(theta.min())\n",
        "        lam_maxs.append(theta.max())\n",
        "\n",
        "    density /= n_probes\n",
        "    approx_lam_min = float(np.median(lam_mins))\n",
        "    approx_lam_max = float(np.median(lam_maxs))\n",
        "    return grid, density, thetas_all, weights_all, approx_lam_min, approx_lam_max\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "output": {
          "id": 786083467206562,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from llama_xl.quantized_layers import QuantizedParallelEmbedding, QuantizedColumnParallelLinear, QuantizedRowParallelLinear\n",
        "from llama_xl.multiple_bits_quantized_layers import MultiBitsQuantizedRowParallelLinear, MultiBitsQuantizedColumnParallelLinear\n",
        "from callm.core.models.llama_xl.utils_quant import (\n",
        "    AsymQuantizer,\n",
        "    ElasticQuantBinarizerSigned,\n",
        "    ElasticQuantBinarizerUnsigned,\n",
        "    ElasticQuantN2UQ,\n",
        "    SymQuantizer,\n",
        ")\n",
        "\n",
        "def convert_weight(layer, w_bits=2):\n",
        "    real_weights = layer.weight\n",
        "    if type(layer) == QuantizedParallelEmbedding:\n",
        "        if layer.w_bits is not None and layer.w_bits >= 16:\n",
        "            weight = layer.weight\n",
        "        elif layer.w_bits is not None and layer.w_bits >= 4:\n",
        "            weight = SymQuantizer.apply(\n",
        "                real_weights, layer.weight_clip_val, layer.w_bits, False\n",
        "            )\n",
        "        elif layer.w_bits == 2:\n",
        "            weight = ElasticQuantN2UQ.apply(\n",
        "                real_weights,\n",
        "                layer.weight_clip_val,\n",
        "                layer.w_bits,\n",
        "                False,\n",
        "            )\n",
        "        else:\n",
        "            weight = ElasticQuantBinarizerSigned.apply(\n",
        "                real_weights,\n",
        "                layer.weight_clip_val,\n",
        "                layer.w_bits,\n",
        "                False,\n",
        "            )\n",
        "    elif type(layer) == QuantizedColumnParallelLinear:\n",
        "        if layer.w_bits is None or layer.w_bits >= 16:\n",
        "            weight = layer.weight\n",
        "        elif layer.w_bits is not None and layer.w_bits > 4:\n",
        "            weight = SymQuantizer.apply(\n",
        "                real_weights,\n",
        "                layer.weight_clip_val,\n",
        "                layer.w_bits,\n",
        "                layer.weight_layerwise,\n",
        "            )\n",
        "        elif layer.w_bits is not None and (layer.w_bits == 2 or layer.w_bits == 0 or layer.w_bits == 1):\n",
        "            weight = ElasticQuantN2UQ.apply(\n",
        "                real_weights,\n",
        "                layer.weight_clip_val,\n",
        "                layer.w_bits,\n",
        "                layer.weight_layerwise,\n",
        "            )\n",
        "        else:\n",
        "            weight = ElasticQuantBinarizerSigned.apply(\n",
        "                real_weights,\n",
        "                layer.weight_clip_val,\n",
        "                layer.w_bits,\n",
        "                layer.weight_layerwise,\n",
        "            )\n",
        "    elif type(layer) == QuantizedRowParallelLinear:\n",
        "        if layer.w_bits is None or layer.w_bits >= 16:\n",
        "            weight = layer.weight\n",
        "        elif layer.w_bits is not None and layer.w_bits > 4:\n",
        "            weight = SymQuantizer.apply(\n",
        "                real_weights, layer.weight_clip_val, layer.w_bits, layer.weight_layerwise\n",
        "            )\n",
        "        elif layer.w_bits is not None and (layer.w_bits == 2 or layer.w_bits == 0 or layer.w_bits == 1):\n",
        "            weight = ElasticQuantN2UQ.apply(\n",
        "                real_weights,\n",
        "                layer.weight_clip_val,\n",
        "                layer.w_bits,\n",
        "                layer.weight_layerwise,\n",
        "            )\n",
        "        else:\n",
        "            weight = ElasticQuantBinarizerSigned.apply(\n",
        "                real_weights,\n",
        "                layer.weight_clip_val,\n",
        "                layer.w_bits,\n",
        "                layer.weight_layerwise,\n",
        "            )\n",
        "    elif type(layer) == MultiBitsQuantizedColumnParallelLinear:\n",
        "        weight_clip_val = layer.weight_clip_val_list[str(int(w_bits))]\n",
        "        if w_bits is None or w_bits >= 16:\n",
        "            weight = layer.weight\n",
        "        elif w_bits is not None and w_bits > 4:\n",
        "            weight = SymQuantizer.apply(\n",
        "                real_weights,\n",
        "                weight_clip_val,\n",
        "                w_bits,\n",
        "                layer.weight_layerwise,\n",
        "            )\n",
        "        elif w_bits is not None and (w_bits == 2 or w_bits == 0 or w_bits == 1):\n",
        "            weight = ElasticQuantN2UQ.apply(\n",
        "                real_weights,\n",
        "                weight_clip_val,\n",
        "                w_bits,\n",
        "                layer.weight_layerwise,\n",
        "            )\n",
        "        else:\n",
        "            weight = ElasticQuantBinarizerSigned.apply(\n",
        "                real_weights,\n",
        "                weight_clip_val,\n",
        "                w_bits,\n",
        "                layer.weight_layerwise,\n",
        "            )\n",
        "    elif type(layer) == MultiBitsQuantizedRowParallelLinear:\n",
        "        weight_clip_val = layer.weight_clip_val_list[str(int(w_bits))]\n",
        "        if w_bits is None or w_bits >= 16:\n",
        "            weight = layer.weight\n",
        "        elif w_bits is not None and w_bits > 4:\n",
        "            weight = SymQuantizer.apply(\n",
        "                real_weights, weight_clip_val, w_bits, layer.weight_layerwise\n",
        "            )\n",
        "        elif w_bits is not None and (w_bits == 2 or w_bits == 0 or w_bits == 1):\n",
        "            weight = ElasticQuantN2UQ.apply(\n",
        "                real_weights,\n",
        "                weight_clip_val,\n",
        "                w_bits,\n",
        "                layer.weight_layerwise,\n",
        "            )\n",
        "        else:\n",
        "            weight = ElasticQuantBinarizerSigned.apply(\n",
        "                real_weights,\n",
        "                weight_clip_val,\n",
        "                w_bits,\n",
        "                layer.weight_layerwise,\n",
        "            )\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    layer.weight.data = weight\n",
        "\n",
        "def post_training(model):\n",
        "    named_layers = get_layers(model)\n",
        "    for name, layer in named_layers.items():\n",
        "        convert_weight(layer, w_bits=4)\n",
        "\n",
        "\n",
        "post_training(model)\n",
        "quantized_state_dict = model.model.state_dict()\n",
        "quantized_state_dict = {key: val.detach().cpu() for key, val in quantized_state_dict.items()}\n",
        "def interpolate_two_state_dict(state_dict1, state_dict2, alpha):\n",
        "    state_dict = {}\n",
        "    for key in state_dict1.keys():\n",
        "        if key in state_dict2.keys():\n",
        "            state_dict[key] = (1 - alpha) * state_dict1[key].clone() + alpha * state_dict2[key].clone()\n",
        "        else:\n",
        "            print(\"Key {} not found in state_dict2\".format(key))\n",
        "    return state_dict\n",
        "\n",
        "alpha = 0\n",
        "new_state_dict = interpolate_two_state_dict(state_dict, quantized_state_dict, alpha)\n",
        "new_state_dict = {key: val.to(device) for key, val in new_state_dict.items()}\n",
        "model.model.load_state_dict(new_state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "output": {
          "id": 1418629896404187,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FP model loss: 3.0466 +/- 0.2331\n"
          ]
        }
      ],
      "source": [
        "fp_model_loss, fp_model_loss_std = evaluate_loss(model, train_data, device, maximum_count = 200)\n",
        "print(\"FP model loss: {:.4f} +/- {:.4f}\".format(fp_model_loss, fp_model_loss_std))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "output": {
          "id": 747360624594128,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "error norm: 0.0000\n",
            "weight norm: 636.4101\n",
            "error ratio: 0.0000\n"
          ]
        }
      ],
      "source": [
        "keys = [key for key in state_dict.keys() if \"layers\" in key]\n",
        "\n",
        "quantized_errors = {key: [] for key in keys}\n",
        "weight_norms = {key: [] for key in keys}\n",
        "\n",
        "def get_quantize_error(state_dict_1, state_dict_2, key):\n",
        "    if key not in state_dict_1.keys() or key not in state_dict_2.keys():\n",
        "        return 0, 0, 0\n",
        "    return torch.norm(state_dict_1[key] - state_dict_2[key]).cpu().item(), torch.norm(state_dict_1[key]).cpu().item(), torch.norm(state_dict_2[key]).cpu().item()\n",
        "\n",
        "new_state_dict = model.model.state_dict()\n",
        "new_state_dict = {key: val.to(\"cpu\") for key, val in new_state_dict.items()}\n",
        "\n",
        "for key in keys:\n",
        "    gap, norm_latent_weights, norm_quantized_weights = get_quantize_error(state_dict, new_state_dict, key)\n",
        "    quantized_errors[key].append(gap)\n",
        "    weight_norms[key].append(norm_latent_weights)\n",
        "\n",
        "quantized_errors = np.array(list(quantized_errors.values()))\n",
        "weight_norms = np.array(list(weight_norms.values()))\n",
        "print(\"error norm: {:.4f}\".format(np.sqrt(np.sum(np.square(quantized_errors)))))\n",
        "print(\"weight norm: {:.4f}\".format(np.sqrt(np.sum(np.square(weight_norms)))))\n",
        "print(\"error ratio: {:.4f}\".format(\n",
        "    np.sqrt(np.sum(np.square(quantized_errors)))/np.sqrt(np.sum(np.square(weight_norms)))\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_probes = 20      # increase for smoother/less noisy density\n",
        "m_steps  = 60      # Lanczos steps (resolution); 50â€“100 is common\n",
        "sigma    = 5e-3    # kernel bandwidth for smoothing; tune per scale\n",
        "\n",
        "for i, batch in enumerate(train_data):\n",
        "    batch = move_batch_to_device(batch, device)\n",
        "    outputs = model(input_ids=batch['input_ids'], labels=batch['labels'])\n",
        "    loss = outputs.loss\n",
        "\n",
        "    grid, dens, thetas_all, weights_all, lam_min, lam_max = slq_density(model, loss, n_probes=n_probes, m=m_steps, sigma=sigma)\n",
        "\n",
        "    np.save(f\"xx/notebooks/hessian_spectrum/1_bits_steps_40k_alpha_0.0/grid_{i}.npy\", grid)\n",
        "    np.save(f\"xx/notebooks/hessian_spectrum/1_bits_steps_40k_alpha_0.0/density_{i}.npy\", dens)\n",
        "    np.save(f\"xx/notebooks/hessian_spectrum/1_bits_steps_40k_alpha_0.0/thetas_{i}.npy\", thetas_all)\n",
        "    np.save(f\"xx/notebooks/hessian_spectrum/1_bits_steps_40k_alpha_0.0/weights_{i}.npy\", weights_all)\n",
        "    np.save(f\"xx/notebooks/hessian_spectrum/1_bits_steps_40k_alpha_0.0/min_and_max_{i}.npy\", np.array([lam_min, lam_max]))\n",
        "    if i >= 9: break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute gradient norm\n",
        "for step in [0, 5000] + list(np.arange(10000, 80001, 10000)):\n",
        "    if step == 0:\n",
        "        data_args.checkpoint_path = \"xx/baselines/full_precision_models/original_xl/llama/1B/\"\n",
        "    else:\n",
        "        data_args.checkpoint_path = f\"xx/1_16_16/paretoq_lr_2e5/{int(step)}/\"\n",
        "    model = load_model_xl()\n",
        "\n",
        "    count = 0\n",
        "    gradient_norms =[]\n",
        "    param_list = get_model_param_list(model)\n",
        "    weight_norm = 0\n",
        "    for param in param_list:\n",
        "        weight_norm += torch.norm(param).item()**2\n",
        "    weight_norm = weight_norm ** 0.5\n",
        "    print(weight_norm)\n",
        "\n",
        "    for batch in train_data:\n",
        "        batch = move_batch_to_device(batch, device)\n",
        "        outputs = model(input_ids=batch['input_ids'], labels=batch['labels'])\n",
        "        loss = outputs.loss\n",
        "        gradient = torch.autograd.grad(loss, param_list, create_graph=False, retain_graph=False)\n",
        "        gradient = [g.view(-1) for g in gradient]\n",
        "        print(\"Gradient norm: {:.4f}\".format(torch.norm(torch.concatenate(gradient)).item()))\n",
        "        gradient_norms.append(torch.norm(torch.concatenate(gradient)).item())\n",
        "        count += 1\n",
        "        if count >= 20:\n",
        "            break\n",
        "\n",
        "    gradient_norms = np.array(gradient_norms)/weight_norm\n",
        "    print(np.mean(gradient_norms), np.std(gradient_norms))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.nn.attention import sdpa_kernel, SDPBackend\n",
        "\n",
        "pathmgr = utils.get_path_manager(\n",
        "        max_parallel=data_args.max_parallel_files,\n",
        "        api_key=data_args.api_key,\n",
        "    )\n",
        "\n",
        "def get_consolidated_ckpt_path(\n",
        "    ckpt_dir: str,\n",
        "    mp_rank: int = 0,\n",
        "    mp_size: int = 1,\n",
        "    parallel_impl = None\n",
        ") -> str:\n",
        "    if mp_size == 1:\n",
        "        assert mp_rank == 0\n",
        "        return os.path.join(ckpt_dir, \"consolidated.pth\")\n",
        "    else:\n",
        "        return os.path.join(ckpt_dir, f\"consolidated.{mp_rank:02d}.pth\")\n",
        "\n",
        "def get_state_dict(path):\n",
        "    mp_rank, mp_size = 0, 1\n",
        "    path = pathmgr.get_local_path(\n",
        "        get_consolidated_ckpt_path(\n",
        "            none_throws(path),\n",
        "            mp_rank,\n",
        "            mp_size,\n",
        "        )\n",
        "    )\n",
        "    state_dict = torch.load(path, map_location=model.device)\n",
        "    return state_dict\n",
        "\n",
        "def interpolate_two_state_dict(state_dict1, state_dict2, alpha):\n",
        "    state_dict = {}\n",
        "    for key in state_dict1.keys():\n",
        "        if key in state_dict2.keys():\n",
        "            state_dict[key] = (1 - alpha) * state_dict1[key].clone() + alpha * state_dict2[key].clone()\n",
        "        else:\n",
        "            print(\"Key {} not found in state_dict2\".format(key))\n",
        "    return state_dict\n",
        "\n",
        "def compute_hessian_on_current_model(model, train_data, device, maximum_count=20):\n",
        "    # compute the hessian statistics\n",
        "    traces = []; eigenvalues = []\n",
        "    count = 0\n",
        "    with sdpa_kernel(SDPBackend.MATH):\n",
        "        for batch in train_data:\n",
        "            batch = move_batch_to_device(batch, device)\n",
        "            outputs = model(input_ids=batch['input_ids'], labels=batch['labels'])\n",
        "            loss = outputs.loss\n",
        "            cur_layer_traces = compute_hessian_traces(model, loss, device)\n",
        "            # cur_eigenvalues, _ = compute_eigenvalue(model, loss, device, top_n=1)\n",
        "\n",
        "            traces.append(cur_layer_traces)\n",
        "            # eigenvalues.append(np.array(cur_eigenvalues[0]))\n",
        "\n",
        "            count += 1\n",
        "            if count >= maximum_count:\n",
        "                break\n",
        "\n",
        "    traces = np.array(traces)\n",
        "    # eigenvalues = np.array(eigenvalues)\n",
        "    print(\"Embedding layer avg trace value {:.2f} +/- {:.2f}\".format(np.mean(traces[:, 0]), np.std(traces[:, 0])))\n",
        "    print(\"Transformer layers avg trace value {:.2f} +/- {:.2f}\".format(np.mean(traces[:, 1:].sum(axis=-1)), np.std(traces[:, 1:].sum(axis=-1))))\n",
        "    print(\"Avg trace value {:.2f} +/- {:.2f}\".format(np.mean(traces.sum(axis=-1)), np.std(traces.sum(axis=-1))))\n",
        "    # \"Avg top eigenvalue {:.2f} +/- {:.2f}\".format(np.mean(eigenvalues.sum(axis=-1)), np.std(eigenvalues.sum(axis=-1)))\n",
        "    return traces, eigenvalues\n",
        "\n",
        "model.training=False # enable standard loss computation\n",
        "\n",
        "traces, eigenvalues = compute_hessian_on_current_model(model, train_data, device, maximum_count=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Literal, Tuple, Optional, Union\n",
        "import numpy as np\n",
        "\n",
        "ArrayLike = Union[np.ndarray, list, tuple]\n",
        "\n",
        "def _trapz(y: np.ndarray, x: np.ndarray) -> float:\n",
        "    \"\"\"Scalar trapezoidal integral; assumes x is 1D, y is broadcastable to x.\"\"\"\n",
        "    return float(np.trapz(y, x))\n",
        "\n",
        "def _validate_grid(grid: np.ndarray, positive_required: bool = False) -> None:\n",
        "    if grid.ndim != 1:\n",
        "        raise ValueError(\"grid must be a 1D array\")\n",
        "    if np.any(~np.isfinite(grid)):\n",
        "        raise ValueError(\"grid contains non-finite values\")\n",
        "    if np.any(np.diff(grid) <= 0):\n",
        "        raise ValueError(\"grid must be strictly increasing\")\n",
        "    if positive_required and np.any(grid <= 0):\n",
        "        raise ValueError(\"log-space requires grid > 0 (since x = log(lambda))\")\n",
        "\n",
        "def _to_2d(a: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Ensure densities are 2D: (n_samples, G).\"\"\"\n",
        "    if a.ndim == 1:\n",
        "        return a[None, :]\n",
        "    if a.ndim == 2:\n",
        "        return a\n",
        "    raise ValueError(\"densities must be 1D or 2D (n_samples, G)\")\n",
        "\n",
        "def normalize_density(\n",
        "    grid: ArrayLike,\n",
        "    density: ArrayLike,\n",
        "    space: Literal[\"linear\", \"log\"] = \"linear\",\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Normalize density curves to integrate to 1 in the appropriate variable.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    grid : array (G,)\n",
        "        Î» grid if space='linear'; Î» grid (still Î») if space='log' (we take x = log(Î»)).\n",
        "    density : array (G,) or (n, G)\n",
        "        p(Î») if space='linear'; q(x) if space='log', where x = log(Î»).\n",
        "    space : {'linear','log'}\n",
        "        Interpretation of 'density' and the integration variable.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    density_norm : array (n, G)\n",
        "        Normalized densities.\n",
        "    grid_out : array (G,)\n",
        "        Echo of grid (Î» grid).\n",
        "    \"\"\"\n",
        "    grid = np.asarray(grid, dtype=float)\n",
        "    dens = np.asarray(density, dtype=float)\n",
        "    dens = _to_2d(dens)\n",
        "\n",
        "    if space == \"linear\":\n",
        "        _validate_grid(grid, positive_required=False)\n",
        "        # Integrate p(Î») dÎ»\n",
        "        masses = np.array([_trapz(dens[i], grid) for i in range(dens.shape[0])])\n",
        "    elif space == \"log\":\n",
        "        _validate_grid(grid, positive_required=True)\n",
        "        x = np.log(grid)\n",
        "        # Integrate q(x) dx\n",
        "        masses = np.array([_trapz(dens[i], x) for i in range(dens.shape[0])])\n",
        "    else:\n",
        "        raise ValueError(\"space must be 'linear' or 'log'\")\n",
        "\n",
        "    # Avoid division by zero\n",
        "    masses[masses == 0] = np.nan\n",
        "    dens_norm = dens / masses[:, None]\n",
        "    return dens_norm, grid\n",
        "\n",
        "def spectral_moment(\n",
        "    grid: ArrayLike,\n",
        "    density: ArrayLike,\n",
        "    k: int = 1,\n",
        "    space: Literal[\"linear\", \"log\"] = \"linear\",\n",
        "    normalize: bool = True,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute the k-th spectral moment Î¼_k = âˆ« Î»^k p(Î») dÎ» (linear space)\n",
        "    or Î¼_k = âˆ« Î»^k q(x) dx with x=log(Î») and Î»=e^x (log space).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    grid : array (G,)\n",
        "        Î» grid (must be strictly increasing).\n",
        "    density : array (G,) or (n, G)\n",
        "        p(Î») if space='linear'; q(x) if space='log'.\n",
        "    k : int\n",
        "        Moment order (k=1 gives âˆ« Î» p(Î») dÎ»).\n",
        "    space : {'linear','log'}\n",
        "        Whether the provided density is with respect to dÎ» ('linear')\n",
        "        or dx where x=log(Î») ('log').\n",
        "    normalize : bool\n",
        "        If True, first normalize each density to integrate to 1 in its space.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    moments : array (n,)\n",
        "        The k-th moment for each density.\n",
        "    \"\"\"\n",
        "    grid = np.asarray(grid, dtype=float)\n",
        "    dens = np.asarray(density, dtype=float)\n",
        "    dens = _to_2d(dens)\n",
        "\n",
        "    if normalize:\n",
        "        dens, grid = normalize_density(grid, dens, space=space)\n",
        "\n",
        "    if space == \"linear\":\n",
        "        # Î¼_k = âˆ« Î»^k p(Î») dÎ»\n",
        "        integrands = dens * (grid[None, :] ** k)\n",
        "        moments = np.array([_trapz(integrands[i], grid) for i in range(dens.shape[0])])\n",
        "    elif space == \"log\":\n",
        "        # Î¼_k = âˆ« (e^x)^k q(x) dx = âˆ« e^{k x} q(x) dx\n",
        "        x = np.log(grid)\n",
        "        integrands = dens * np.exp(k * x[None, :])\n",
        "        moments = np.array([_trapz(integrands[i], x) for i in range(dens.shape[0])])\n",
        "    else:\n",
        "        raise ValueError(\"space must be 'linear' or 'log'\")\n",
        "    return moments\n",
        "\n",
        "def trace_from_density(\n",
        "    grid: ArrayLike,\n",
        "    density: ArrayLike,\n",
        "    matrix_size: int,\n",
        "    space: Literal[\"linear\", \"log\"] = \"linear\",\n",
        "    normalize: bool = True,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute tr(H) from discretized spectrum on a grid:\n",
        "        tr(H) = n * âˆ« Î» p(Î») dÎ»  (linear space)\n",
        "        tr(H) = n * âˆ« e^x q(x) dx (log space, x = log Î»)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    grid : array (G,)\n",
        "        Î» grid (strictly increasing).\n",
        "    density : array (G,) or (n, G)\n",
        "        p(Î») if space='linear'; q(x) if space='log'.\n",
        "    matrix_size : int\n",
        "        n = dimension of H (number of eigenvalues).\n",
        "    space : {'linear','log'}\n",
        "        Interpretation of density.\n",
        "    normalize : bool\n",
        "        Normalize each density before computing the moment.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    traces : array (n,)\n",
        "        Estimated traces for each provided density.\n",
        "    \"\"\"\n",
        "    moments1 = spectral_moment(grid, density, k=1, space=space, normalize=normalize)\n",
        "    return matrix_size * moments1\n",
        "\n",
        "\n",
        "param_list = get_model_param_list(model)\n",
        "dim = sum(p.numel() for p in param_list)\n",
        "trace_from_density(grid, dens, dim, normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot density\n",
        "# --- Plot the estimated spectral density ---\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.scatter(grid, np.log(dens))\n",
        "\n",
        "# plt.axvline(lam_min, linestyle=\"--\", label=\"~ Î»_min (Ritz median)\")\n",
        "# plt.axvline(lam_max, linestyle=\"--\", label=\"~ Î»_max (Ritz median)\")\n",
        "# plt.xticks(np.arange(-1, 1.1, 0.5))\n",
        "plt.xlabel(\"Eigenvalue Î»\")\n",
        "plt.ylabel(\"Estimated density Ï(Î»)\")\n",
        "plt.title(\"Hessian spectrum (of the embedding layers)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot density\n",
        "# --- Plot the estimated spectral density ---\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.scatter(grid, np.log(dens), label=\"SLQ density estimate\")\n",
        "# plt.axvline(lam_min, linestyle=\"--\", label=\"~ Î»_min (Ritz median)\")\n",
        "# plt.axvline(lam_max, linestyle=\"--\", label=\"~ Î»_max (Ritz median)\")\n",
        "plt.xlabel(\"Eigenvalue Î»\")\n",
        "plt.ylabel(\"Estimated density Ï(Î»)\")\n",
        "plt.title(\"Hessian spectrum (SLQ approximation)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.save(\"xx/notebooks/slq_grid_2bit_10k.npy\", grid)\n",
        "np.save(\"xx/notebooks/slq_density_2bit_10k.npy\", dens)\n",
        "np.save(\"xx/notebooks/slq_ritz_all_2bit_10k.npy\", ritz_all)\n",
        "lam_min, lam_max\n",
        "# 1-bit (-15.938819445322734, 16.789593178943367)\n",
        "# (-127.41929134942242, 123.44032548997839)\n",
        "# 2-bit (-69.01394941459208, 66.09353891130456)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid = np.load(\"xx/notebooks/hessian_spectrum/slq_grid_2bit.npy\")\n",
        "dens = np.load(\"xx/notebooks/hessian_spectrum/slq_density_2bit.npy\")\n",
        "ritz_all = np.load(\"xx/notebooks/hessian_spectrum/slq_ritz_all_2bit.npy\")"
      ]
    }
  ],
  "metadata": {
    "fileHeader": "",
    "fileUid": "6e298e15-3231-4b34-900b-fcf622ada168",
    "isAdHoc": false,
    "kernelspec": {
      "display_name": "core_ai_asl",
      "language": "python",
      "name": "bento_kernel_core_ai_asl"
    },
    "language_info": {
      "name": "plaintext"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
